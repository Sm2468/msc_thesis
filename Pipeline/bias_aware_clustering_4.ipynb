{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd017b090ea87be7374e1a24313da865c5437f7b271e131ad745a67a82a8cb610e1",
   "display_name": "Python 3.9.1 32-bit ('clusterenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(r'C:\\Users\\Gebruiker\\Documents\\thesiscode\\experimental\\preprocessed_wine.csv', index_col=0)\n",
    "errors = all_data['errors']\n",
    "clustering_features_preprocessed = all_data.drop(['predicted_class', 'true_class', 'errors'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
       "12  0.687846   -0.160373  0.141557          -0.694417  -0.834587   \n",
       "23 -0.356751   -0.343481  0.502356          -0.101678  -0.381526   \n",
       "\n",
       "    total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "12       0.004312    0.267992             -0.318793         0.017405   \n",
       "23      -0.222556   -0.278564             -0.635733        -0.650337   \n",
       "\n",
       "    color_intensity       hue  od280/od315_of_diluted_wines   proline  \\\n",
       "12         0.697247  0.405843                     -0.146241  1.256507   \n",
       "23        -0.367414 -0.033392                      1.394286  0.408775   \n",
       "\n",
       "    clusters  new_clusters  true_class  predicted_class  errors  \n",
       "12         1           0.0         0.0                0     0.0  \n",
       "23         1           0.0         0.0                1     1.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>clusters</th>\n      <th>new_clusters</th>\n      <th>true_class</th>\n      <th>predicted_class</th>\n      <th>errors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>0.687846</td>\n      <td>-0.160373</td>\n      <td>0.141557</td>\n      <td>-0.694417</td>\n      <td>-0.834587</td>\n      <td>0.004312</td>\n      <td>0.267992</td>\n      <td>-0.318793</td>\n      <td>0.017405</td>\n      <td>0.697247</td>\n      <td>0.405843</td>\n      <td>-0.146241</td>\n      <td>1.256507</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>-0.356751</td>\n      <td>-0.343481</td>\n      <td>0.502356</td>\n      <td>-0.101678</td>\n      <td>-0.381526</td>\n      <td>-0.222556</td>\n      <td>-0.278564</td>\n      <td>-0.635733</td>\n      <td>-0.650337</td>\n      <td>-0.367414</td>\n      <td>-0.033392</td>\n      <td>1.394286</td>\n      <td>0.408775</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "to_scale = ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
    "clustering_features_preprocessed[to_scale] = StandardScaler().fit_transform(clustering_features_preprocessed[to_scale])\n",
    "\n",
    "clustering_features[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is used to calculate the F-score of the instances in the clusters.\n",
    "# the lower the f score is, the worse the performance is \n",
    "\n",
    "# requires the all_data dataframe (with predicted and true class, but the errors col is not needed)\n",
    "def F_score(results, class_number):\n",
    "    true_pos = results.loc[results[\"true_class\"] == class_number][results[\"predicted_class\"] == class_number]\n",
    "    true_neg = results.loc[results[\"true_class\"] != class_number][results[\"predicted_class\"] != class_number]\n",
    "    false_pos = results.loc[results[\"true_class\"] != class_number][results[\"predicted_class\"] == class_number]\n",
    "    false_neg = results.loc[results[\"true_class\"] == class_number][results[\"predicted_class\"] != class_number]\n",
    "    \n",
    "    try:\n",
    "        precision =  len(true_pos)/(len(true_pos) + len(false_pos))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    try:\n",
    "        recall = len(true_pos)/(len(true_pos) + len(false_neg))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "    # TODO solve the \"float division by zero\" error\n",
    "    try:\n",
    "        f_score = 2 * ((precision * recall)/(precision + recall))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "    return f_score\n",
    "\n",
    "# Calculating the macro average F-score --> will eventually be replaced with weighted F-score\n",
    "def mean_f_score(results):\n",
    "    classes = results['true_class'].unique()\n",
    "    class_list = []\n",
    "    for i in classes:\n",
    "        class_i = F_score(results, i)\n",
    "        class_list.append(class_i)\n",
    "    mean_f_score = (sum(class_list))/len(classes)\n",
    "    print('this is the mean F-score of all classes within this cluster in the list: ', mean_f_score)\n",
    "\n",
    "    return(mean_f_score)\n",
    "\n",
    "# Calculating the bias for each cluster\n",
    "def calculate_bias(clustered_data, cluster_number):\n",
    "    cluster_x = clustered_data.loc[clustered_data[\"clusters\"] == cluster_number]\n",
    "    remaining_clusters = clustered_data.loc[clustered_data[\"clusters\"] != cluster_number]\n",
    "    \n",
    "    # Bias definition: \n",
    "    return mean_f_score(remaining_clusters) - mean_f_score(cluster_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_model_kwargs = {\n",
    "    \"init\": \"k-means++\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the variances of the errors column\n",
    "def calculate_variance(data):\n",
    "    # Receives the data within one cluster to calculate the variance\n",
    "\n",
    "    # Obtain errors column\n",
    "    errors_col = data['errors']\n",
    "    # Number of observations\n",
    "    n = len(data)\n",
    "    # Mean of the data\n",
    "    mean = sum(errors_col)/n\n",
    "    # Squared deviation \n",
    "    deviations = [(x - mean) ** 2 for x in errors_col]\n",
    "    # Variance\n",
    "    variance = sum(deviations) / n\n",
    "    return variance\n",
    "\n",
    "def get_highest_var_cluster(data):\n",
    "    clusters = data['clusters'].unique()\n",
    "    highest_variance = 0\n",
    "    best_cluster = None\n",
    "    cluster_number = None\n",
    "    for i in clusters:\n",
    "        print('this is i:', i)\n",
    "        cluster_i = data[data['clusters'] == i]\n",
    "        variance_cluster = calculate_variance(cluster_i)\n",
    "        print('variance cluster:', variance_cluster)\n",
    "        print('highest variance:', highest_variance)\n",
    "\n",
    "        if variance_cluster > highest_variance:\n",
    "            highest_variance = variance_cluster\n",
    "            best_cluster = cluster_i\n",
    "            cluster_number = i\n",
    "            print('this is the cluster number:', cluster_number)\n",
    "\n",
    "    return cluster_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "12     13.75        1.73  2.41               16.0       89.0           2.60   \n",
       "23     12.85        1.60  2.52               17.8       95.0           2.48   \n",
       "25     13.05        2.05  3.22               25.0      124.0           2.63   \n",
       "35     13.48        1.81  2.41               20.5      100.0           2.70   \n",
       "13     14.75        1.73  2.39               11.4       91.0           3.10   \n",
       "65     12.37        1.21  2.56               18.1       98.0           2.42   \n",
       "48     14.10        2.02  2.40               18.8      103.0           2.75   \n",
       "78     12.33        0.99  1.95               14.8      136.0           1.90   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "6      14.39        1.87  2.45               14.6       96.0           2.50   \n",
       "42     13.88        1.89  2.59               15.0      101.0           3.25   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "29     14.02        1.68  2.21               16.0       96.0           2.65   \n",
       "45     14.21        4.04  2.44               18.9      111.0           2.85   \n",
       "106    12.25        1.73  2.12               19.0       80.0           1.65   \n",
       "94     11.62        1.99  2.28               18.0       98.0           3.02   \n",
       "5      14.20        1.76  2.45               15.2      112.0           3.27   \n",
       "53     13.77        1.90  2.68               17.1      115.0           3.00   \n",
       "93     12.29        2.83  2.22               18.0       88.0           2.45   \n",
       "41     13.41        3.84  2.12               18.8       90.0           2.45   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "12         2.76                  0.29             1.81             5.60  1.15   \n",
       "23         2.37                  0.26             1.46             3.93  1.09   \n",
       "25         2.68                  0.47             1.92             3.58  1.13   \n",
       "35         2.98                  0.26             1.86             5.10  1.04   \n",
       "13         3.69                  0.43             2.81             5.40  1.25   \n",
       "65         2.65                  0.37             2.08             4.60  1.19   \n",
       "48         2.92                  0.32             2.38             6.20  1.07   \n",
       "78         1.85                  0.35             2.76             3.40  1.06   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "6          2.52                  0.30             1.98             5.25  1.02   \n",
       "42         3.56                  0.17             1.70             5.43  0.88   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "29         2.33                  0.26             1.98             4.70  1.04   \n",
       "45         2.65                  0.30             1.25             5.24  0.87   \n",
       "106        2.03                  0.37             1.63             3.40  1.00   \n",
       "94         2.26                  0.17             1.35             3.25  1.16   \n",
       "5          3.39                  0.34             1.97             6.75  1.05   \n",
       "53         2.79                  0.39             1.68             6.30  1.13   \n",
       "93         2.25                  0.25             1.99             2.15  1.15   \n",
       "41         2.68                  0.27             1.48             4.28  0.91   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  predicted_class  true_class  \\\n",
       "12                           2.90   1320.0                0         0.0   \n",
       "23                           3.63   1015.0                1         0.0   \n",
       "25                           3.20    830.0                0         0.0   \n",
       "35                           3.47    920.0                1         0.0   \n",
       "13                           2.73   1150.0                2         0.0   \n",
       "65                           2.30    678.0                0         1.0   \n",
       "48                           2.75   1060.0                2         0.0   \n",
       "78                           2.31    750.0                1         1.0   \n",
       "3                            3.45   1480.0                0         0.0   \n",
       "6                            3.58   1290.0                1         0.0   \n",
       "42                           3.56   1095.0                1         0.0   \n",
       "2                            3.17   1185.0                2         0.0   \n",
       "29                           3.59   1035.0                0         0.0   \n",
       "45                           3.33   1080.0                1         0.0   \n",
       "106                          3.17    510.0                1         1.0   \n",
       "94                           2.96    345.0                2         1.0   \n",
       "5                            2.85   1450.0                0         0.0   \n",
       "53                           2.93   1375.0                1         0.0   \n",
       "93                           3.30    290.0                1         1.0   \n",
       "41                           3.00   1035.0                2         0.0   \n",
       "\n",
       "     errors  \n",
       "12      0.0  \n",
       "23      1.0  \n",
       "25      0.0  \n",
       "35      1.0  \n",
       "13      1.0  \n",
       "65      1.0  \n",
       "48      1.0  \n",
       "78      0.0  \n",
       "3       0.0  \n",
       "6       1.0  \n",
       "42      1.0  \n",
       "2       1.0  \n",
       "29      0.0  \n",
       "45      1.0  \n",
       "106     0.0  \n",
       "94      1.0  \n",
       "5       0.0  \n",
       "53      1.0  \n",
       "93      0.0  \n",
       "41      1.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>predicted_class</th>\n      <th>true_class</th>\n      <th>errors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>13.75</td>\n      <td>1.73</td>\n      <td>2.41</td>\n      <td>16.0</td>\n      <td>89.0</td>\n      <td>2.60</td>\n      <td>2.76</td>\n      <td>0.29</td>\n      <td>1.81</td>\n      <td>5.60</td>\n      <td>1.15</td>\n      <td>2.90</td>\n      <td>1320.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>12.85</td>\n      <td>1.60</td>\n      <td>2.52</td>\n      <td>17.8</td>\n      <td>95.0</td>\n      <td>2.48</td>\n      <td>2.37</td>\n      <td>0.26</td>\n      <td>1.46</td>\n      <td>3.93</td>\n      <td>1.09</td>\n      <td>3.63</td>\n      <td>1015.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>13.05</td>\n      <td>2.05</td>\n      <td>3.22</td>\n      <td>25.0</td>\n      <td>124.0</td>\n      <td>2.63</td>\n      <td>2.68</td>\n      <td>0.47</td>\n      <td>1.92</td>\n      <td>3.58</td>\n      <td>1.13</td>\n      <td>3.20</td>\n      <td>830.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>13.48</td>\n      <td>1.81</td>\n      <td>2.41</td>\n      <td>20.5</td>\n      <td>100.0</td>\n      <td>2.70</td>\n      <td>2.98</td>\n      <td>0.26</td>\n      <td>1.86</td>\n      <td>5.10</td>\n      <td>1.04</td>\n      <td>3.47</td>\n      <td>920.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14.75</td>\n      <td>1.73</td>\n      <td>2.39</td>\n      <td>11.4</td>\n      <td>91.0</td>\n      <td>3.10</td>\n      <td>3.69</td>\n      <td>0.43</td>\n      <td>2.81</td>\n      <td>5.40</td>\n      <td>1.25</td>\n      <td>2.73</td>\n      <td>1150.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>12.37</td>\n      <td>1.21</td>\n      <td>2.56</td>\n      <td>18.1</td>\n      <td>98.0</td>\n      <td>2.42</td>\n      <td>2.65</td>\n      <td>0.37</td>\n      <td>2.08</td>\n      <td>4.60</td>\n      <td>1.19</td>\n      <td>2.30</td>\n      <td>678.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>14.10</td>\n      <td>2.02</td>\n      <td>2.40</td>\n      <td>18.8</td>\n      <td>103.0</td>\n      <td>2.75</td>\n      <td>2.92</td>\n      <td>0.32</td>\n      <td>2.38</td>\n      <td>6.20</td>\n      <td>1.07</td>\n      <td>2.75</td>\n      <td>1060.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>12.33</td>\n      <td>0.99</td>\n      <td>1.95</td>\n      <td>14.8</td>\n      <td>136.0</td>\n      <td>1.90</td>\n      <td>1.85</td>\n      <td>0.35</td>\n      <td>2.76</td>\n      <td>3.40</td>\n      <td>1.06</td>\n      <td>2.31</td>\n      <td>750.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>14.39</td>\n      <td>1.87</td>\n      <td>2.45</td>\n      <td>14.6</td>\n      <td>96.0</td>\n      <td>2.50</td>\n      <td>2.52</td>\n      <td>0.30</td>\n      <td>1.98</td>\n      <td>5.25</td>\n      <td>1.02</td>\n      <td>3.58</td>\n      <td>1290.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>13.88</td>\n      <td>1.89</td>\n      <td>2.59</td>\n      <td>15.0</td>\n      <td>101.0</td>\n      <td>3.25</td>\n      <td>3.56</td>\n      <td>0.17</td>\n      <td>1.70</td>\n      <td>5.43</td>\n      <td>0.88</td>\n      <td>3.56</td>\n      <td>1095.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>14.02</td>\n      <td>1.68</td>\n      <td>2.21</td>\n      <td>16.0</td>\n      <td>96.0</td>\n      <td>2.65</td>\n      <td>2.33</td>\n      <td>0.26</td>\n      <td>1.98</td>\n      <td>4.70</td>\n      <td>1.04</td>\n      <td>3.59</td>\n      <td>1035.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>14.21</td>\n      <td>4.04</td>\n      <td>2.44</td>\n      <td>18.9</td>\n      <td>111.0</td>\n      <td>2.85</td>\n      <td>2.65</td>\n      <td>0.30</td>\n      <td>1.25</td>\n      <td>5.24</td>\n      <td>0.87</td>\n      <td>3.33</td>\n      <td>1080.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>12.25</td>\n      <td>1.73</td>\n      <td>2.12</td>\n      <td>19.0</td>\n      <td>80.0</td>\n      <td>1.65</td>\n      <td>2.03</td>\n      <td>0.37</td>\n      <td>1.63</td>\n      <td>3.40</td>\n      <td>1.00</td>\n      <td>3.17</td>\n      <td>510.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>11.62</td>\n      <td>1.99</td>\n      <td>2.28</td>\n      <td>18.0</td>\n      <td>98.0</td>\n      <td>3.02</td>\n      <td>2.26</td>\n      <td>0.17</td>\n      <td>1.35</td>\n      <td>3.25</td>\n      <td>1.16</td>\n      <td>2.96</td>\n      <td>345.0</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>14.20</td>\n      <td>1.76</td>\n      <td>2.45</td>\n      <td>15.2</td>\n      <td>112.0</td>\n      <td>3.27</td>\n      <td>3.39</td>\n      <td>0.34</td>\n      <td>1.97</td>\n      <td>6.75</td>\n      <td>1.05</td>\n      <td>2.85</td>\n      <td>1450.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>13.77</td>\n      <td>1.90</td>\n      <td>2.68</td>\n      <td>17.1</td>\n      <td>115.0</td>\n      <td>3.00</td>\n      <td>2.79</td>\n      <td>0.39</td>\n      <td>1.68</td>\n      <td>6.30</td>\n      <td>1.13</td>\n      <td>2.93</td>\n      <td>1375.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>12.29</td>\n      <td>2.83</td>\n      <td>2.22</td>\n      <td>18.0</td>\n      <td>88.0</td>\n      <td>2.45</td>\n      <td>2.25</td>\n      <td>0.25</td>\n      <td>1.99</td>\n      <td>2.15</td>\n      <td>1.15</td>\n      <td>3.30</td>\n      <td>290.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>13.41</td>\n      <td>3.84</td>\n      <td>2.12</td>\n      <td>18.8</td>\n      <td>90.0</td>\n      <td>2.45</td>\n      <td>2.68</td>\n      <td>0.27</td>\n      <td>1.48</td>\n      <td>4.28</td>\n      <td>0.91</td>\n      <td>3.00</td>\n      <td>1035.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "# Initialisation of clustering_features\n",
    "clustering_features_preprocessed['clusters'] = 1\n",
    "clustering_features = clustering_features_preprocessed\n",
    "all_data.head(5) #true class doesnt seem to have the right values? I see no 2 or 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Value for x: 1\nnew clustering features before merge:      alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n12  0.687846   -0.160373  0.141557          -0.694417  -0.834587   \n23 -0.356751   -0.343481  0.502356          -0.101678  -0.381526   \n25 -0.124619    0.290352  2.798346           2.269277   1.808272   \n35  0.374467   -0.047692  0.141557           0.787430  -0.003974   \n13  1.848509   -0.160373  0.075958          -2.209194  -0.683567   \n\n    total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n12       0.004312    0.267992             -0.318793         0.017405   \n23      -0.222556   -0.278564             -0.635733        -0.650337   \n25       0.061029    0.155878              1.582845         0.227267   \n35       0.193369    0.576306             -0.635733         0.112797   \n13       0.949596    1.571320              1.160258         1.925240   \n\n    color_intensity       hue  od280/od315_of_diluted_wines   proline  \\\n12         0.697247  0.405843                     -0.146241  1.256507   \n23        -0.367414 -0.033392                      1.394286  0.408775   \n25        -0.590546  0.259431                      0.486852 -0.105424   \n35         0.378486 -0.399421                      1.056636  0.144727   \n13         0.569743  1.137900                     -0.504994  0.784001   \n\n    clusters  \n12         0  \n23         0  \n25         0  \n35         0  \n13         0  \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "     true_class  predicted_class\n12          0.0                0\n23          0.0                1\n25          0.0                0\n35          0.0                1\n13          0.0                2\n65          1.0                0\n48          0.0                2\n78          1.0                1\n3           0.0                0\n6           0.0                1\n42          0.0                1\n2           0.0                2\n29          0.0                0\n45          0.0                1\n106         1.0                1\n94          1.0                2\n5           0.0                0\n53          0.0                1\n93          1.0                1\n41          0.0                2\n54          0.0                1\n24          0.0                0\n64          1.0                2\n28          0.0                0\n89          1.0                0\n92          1.0                1\n79          1.0                0\n14          0.0                2\n44          0.0                2\n66          1.0                2\n85          1.0                2\n99          1.0                1\n57          0.0                0\n71          1.0                2\n11          0.0                0\n36          0.0                2\n62          1.0                2\n0           0.0                0\n27          0.0                0\n98          1.0                2\n20          0.0                1\n77          1.0                1\n30          0.0                2\n17          0.0                0\n59          1.0                2\n21          0.0                0\n55          0.0                2\n16          0.0                0\n91          1.0                0\n100         1.0                1\n74          1.0                0\n87          1.0                2\n90          1.0                0\n84          1.0                2\n18          0.0                0\n97          1.0                1\n61          1.0                2",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-49129ed1a288>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# new_clustering_features = new_clustering_features.join(all_data['true_class'], how='left')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#todo: solve merge/join error here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mnew_clustering_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_clustering_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'true_class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'predicted_class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'true_class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'predicted_class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new clustering features after merge:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_clustering_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# calculate bias for each of the two clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clusterenv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   8193\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8195\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m   8196\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8197\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clusterenv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m ) -> \"DataFrame\":\n\u001b[1;32m---> 74\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clusterenv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clusterenv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1031\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1034\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\clusterenv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1684\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m:      true_class  predicted_class\n12          0.0                0\n23          0.0                1\n25          0.0                0\n35          0.0                1\n13          0.0                2\n65          1.0                0\n48          0.0                2\n78          1.0                1\n3           0.0                0\n6           0.0                1\n42          0.0                1\n2           0.0                2\n29          0.0                0\n45          0.0                1\n106         1.0                1\n94          1.0                2\n5           0.0                0\n53          0.0                1\n93          1.0                1\n41          0.0                2\n54          0.0                1\n24          0.0                0\n64          1.0                2\n28          0.0                0\n89          1.0                0\n92          1.0                1\n79          1.0                0\n14          0.0                2\n44          0.0                2\n66          1.0                2\n85          1.0                2\n99          1.0                1\n57          0.0                0\n71          1.0                2\n11          0.0                0\n36          0.0                2\n62          1.0                2\n0           0.0                0\n27          0.0                0\n98          1.0                2\n20          0.0                1\n77          1.0                1\n30          0.0                2\n17          0.0                0\n59          1.0                2\n21          0.0                0\n55          0.0                2\n16          0.0                0\n91          1.0                0\n100         1.0                1\n74          1.0                0\n87          1.0                2\n90          1.0                0\n84          1.0                2\n18          0.0                0\n97          1.0                1\n61          1.0                2"
     ]
    }
   ],
   "source": [
    "MAX_ITER = 10 # --> in each iteration we split on the cluster with the highest variance\n",
    "x = 1\n",
    "bias_prev_iteration_cluster = 0\n",
    "\n",
    "for i in range(1, MAX_ITER):\n",
    "    print('Value for x:', x)\n",
    "    # select the instances with cluster X as dataset\n",
    "    high_bias_cluster = clustering_features.loc[clustering_features['clusters'] == x]\n",
    "    high_bias_cluster.drop('clusters', axis=1)\n",
    "\n",
    "    # Apply Kmeans on this dataset\n",
    "    kmeans_algo = KMeans(n_clusters=2, **clus_model_kwargs).fit(high_bias_cluster) \n",
    "    high_bias_cluster['new_clusters'] = pd.DataFrame(kmeans_algo.predict(high_bias_cluster),index=high_bias_cluster.index) \n",
    "\n",
    "    # print(\"where is the NaN\")\n",
    "    # print(high_bias_cluster)\n",
    "    # print('amount of rows with Nan:', high_bias_cluster.isna().sum())\n",
    "    \n",
    "    new_clustering_features = clustering_features\n",
    "\n",
    "    new_clustering_features['clusters'] = high_bias_cluster['new_clusters'].combine_first(new_clustering_features['clusters'])\n",
    "    print('new clustering features before merge:', new_clustering_features.head(5))\n",
    "    # new_clustering_features = new_clustering_features.join(all_data[['true_class', 'predicted_class']], how='left')\n",
    "    # new_clustering_features = new_clustering_features.join(all_data['true_class'], how='left')\n",
    "    #todo: solve merge/join error here\n",
    "    new_clustering_features = new_clustering_features.merge(all_data[['true_class', 'predicted_class']], on=all_data[['true_class', 'predicted_class']], how='left')\n",
    "    print('new clustering features after merge:', new_clustering_features.head(5))\n",
    "    # calculate bias for each of the two clusters\n",
    "    negative_bias_0 = calculate_bias(new_clustering_features, 0)\n",
    "    negative_bias_1 = calculate_bias(new_clustering_features, 1)\n",
    "\n",
    "    new_clustering_features.drop(['true_class', 'predicted_class'], axis=1)\n",
    "\n",
    "    if max(negative_bias_0, negative_bias_1) >= bias_prev_iteration_cluster:\n",
    "        # select cluster with highest negative bias and merge new cluster assignments with the df\n",
    "        bias_prev_iteration_cluster = max(negative_bias_0, negative_bias_1)\n",
    "        clustering_features = new_clustering_features\n",
    "    else:\n",
    "        break\n",
    "        print('no clusters with a higher bias are left')\n",
    "    \n",
    "    clustering_features = clustering_features.join(errors, how='left')\n",
    "    x = get_highest_var_cluster(clustering_features)\n",
    "\n",
    "    clustering_features.drop('errors', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # new_clustering_features = clustering_features.join(high_bias_cluster['new_clusters'], how='left')\n",
    "    \n",
    "    # new_clustering_features['clusters'] = new_clustering_features['new_clusters'].combine_first(new_clustering_features['clusters'])\n",
    "    # new_clustering_features.drop('new_clusters', axis=1)"
   ]
  }
 ]
}